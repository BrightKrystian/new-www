{"componentChunkName":"component---src-templates-post-template-tsx","path":"/blog/image-classification-tensorflowlite-android","result":{"data":{"markdownRemark":{"html":"<p>As I've already listed in my recent <a href=\"https://brightinventions.pl/blog/are-we-ready-for-deep-learning-on-mobile-devices/\">blog post</a> there are lots of advantages of making inference directly on a mobile device instead of using cloud solutions. Because of mobile devices' computation limitations, we can't migrate all of the available models to work on mobile. Unfortunately, plenty of them won't work on mobile devices but that's fine because we often don't need these heavy models on mobile devices. In this blog post, we will create a simple Android application that will take advantage of <a href=\"https://arxiv.org/abs/1801.04381\">MobileNetV2</a> that was pre-trained on ImageNet.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/93d5e5f8092ddab1c61b71333f6f586a/77607/surf1.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 57.432432432432435%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAMCBAX/xAAVAQEBAAAAAAAAAAAAAAAAAAABAv/aAAwDAQACEAMQAAABZOmsNEQS/wD/xAAbEAACAwADAAAAAAAAAAAAAAABAgADEQQUM//aAAgBAQABBQIWiC+sTtVTkeiqNCLn/8QAGBEAAgMAAAAAAAAAAAAAAAAAAAECAxL/2gAIAQMBAT8BdsjbP//EABcRAAMBAAAAAAAAAAAAAAAAAAACEwH/2gAIAQIBAT8BiuEFP//EABkQAAIDAQAAAAAAAAAAAAAAAAAxAQIhEP/aAAgBAQAGPwLYNrAuIR//xAAcEAACAgIDAAAAAAAAAAAAAAAAAREhMUEQUfH/2gAIAQEAAT8h2SB86Swx+MTDXXBICp//2gAMAwEAAgADAAAAEAzP/8QAFhEBAQEAAAAAAAAAAAAAAAAAARAh/9oACAEDAQE/EFOy/8QAFhEBAQEAAAAAAAAAAAAAAAAAAQAR/9oACAECAQE/EBWkz//EABsQAQEAAgMBAAAAAAAAAAAAAAERACFBcZGB/9oACAEBAAE/EAE7XT7lS0twv3AIVOIMBMxhRoY5wrNy8c//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"surf1\"\n        title=\"surf1\"\n        src=\"/static/93d5e5f8092ddab1c61b71333f6f586a/1c72d/surf1.jpg\"\n        srcset=\"/static/93d5e5f8092ddab1c61b71333f6f586a/a80bd/surf1.jpg 148w,\n/static/93d5e5f8092ddab1c61b71333f6f586a/1c91a/surf1.jpg 295w,\n/static/93d5e5f8092ddab1c61b71333f6f586a/1c72d/surf1.jpg 590w,\n/static/93d5e5f8092ddab1c61b71333f6f586a/a8a14/surf1.jpg 885w,\n/static/93d5e5f8092ddab1c61b71333f6f586a/fbd2c/surf1.jpg 1180w,\n/static/93d5e5f8092ddab1c61b71333f6f586a/77607/surf1.jpg 2885w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2>Let's make our hands dirty...</h2>\n<p>In our Android app project we need to add TFLite dependency to <code>build.gradle</code> file. </p>\n<pre><code>implementation 'org.tensorflow:tensorflow-lite:1.13.1'\n</code></pre>\n<p>and the undermentioned snippet to prevent compressing the model.</p>\n<pre><code>aaptOptions {  \n    noCompress \"tflite\"  \n    noCompress \"lite\"  \n}\n</code></pre>\n<p>The next step is to get a model for the image classification problem. One way is to create your own or take pre-trained one from <a href=\"https://www.tensorflow.org/lite/guide/hosted_models\">here</a> and put it to the <code>assets</code> folder. We will be using customized pre-trained MobileNetV2 that I've created for the sake of this demo. Our model will be able to or at least it should distinguish üåä <em>kitesurfing, windsurfing</em>, and <em>surfing</em> üèÑ‚Äç‚ôÇÔ∏è.  You can download this model as well as labels from my <a href=\"https://github.com/ares97/tflitedemo-mobilenetv2-imagenet-classification/tree/master/app/src/main/assets\">git repository</a>.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/22f33823a573b4aae8fc3573c2c17e4e/0f98f/kite.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.08108108108109%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAQBAwX/xAAWAQEBAQAAAAAAAAAAAAAAAAABAgP/2gAMAwEAAhADEAAAAXKkJrB8zRP/xAAcEAABAwUAAAAAAAAAAAAAAAABAAISBBAREyH/2gAIAQEAAQUCk4IVGVvUnGxPf//EABcRAQEBAQAAAAAAAAAAAAAAAAEAAxP/2gAIAQMBAT8BdG6N/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQIBAT8BiP/EABkQAAMAAwAAAAAAAAAAAAAAAAABMhAxof/aAAgBAQAGPwKOkslm3n//xAAdEAEBAAEEAwAAAAAAAAAAAAABADERIUFxgZHh/9oACAEBAAE/IdDk9CLD2Ej7EE3bzEnK/9oADAMBAAIAAwAAABBgz//EABYRAAMAAAAAAAAAAAAAAAAAABARIf/aAAgBAwEBPxByB//EABYRAQEBAAAAAAAAAAAAAAAAAAABEf/aAAgBAgEBPxCQw//EABkQAQADAQEAAAAAAAAAAAAAAAEAESExUf/aAAgBAQABPxA4BL2Adawp7SxoSILj21EgI0y+1P/Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"kite\"\n        title=\"kite\"\n        src=\"/static/22f33823a573b4aae8fc3573c2c17e4e/1c72d/kite.jpg\"\n        srcset=\"/static/22f33823a573b4aae8fc3573c2c17e4e/a80bd/kite.jpg 148w,\n/static/22f33823a573b4aae8fc3573c2c17e4e/1c91a/kite.jpg 295w,\n/static/22f33823a573b4aae8fc3573c2c17e4e/1c72d/kite.jpg 590w,\n/static/22f33823a573b4aae8fc3573c2c17e4e/a8a14/kite.jpg 885w,\n/static/22f33823a573b4aae8fc3573c2c17e4e/fbd2c/kite.jpg 1180w,\n/static/22f33823a573b4aae8fc3573c2c17e4e/0f98f/kite.jpg 1920w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h3>Dive into the code</h3>\n<p>In order to make use of the prepared model we need to somehow import it into code. Let's use <code>tf.lite.Interpreter</code>   interface for the model. </p>\n<p>You can set up an interpreter in many ways, one recommended on <a href=\"https://www.tensorflow.org/lite/models/image_classification/android\">TF website</a> is to make use of <code>MappedByteBuffer</code>.</p>\n<pre><code class=\"language-kotlin\">@Throws(IOException::class)  \nprivate fun getModelByteBuffer(assetManager: AssetManager, modelPath: String): MappedByteBuffer {  \n    val fileDescriptor = assetManager.openFd(modelPath)  \n    val inputStream = FileInputStream(fileDescriptor.fileDescriptor)  \n    val fileChannel = inputStream.channel  \n    val startOffset = fileDescriptor.startOffset  \n    val declaredLength = fileDescriptor.declaredLength  \n    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength) \n}\n</code></pre>\n<p>and then...</p>\n<pre><code class=\"language-kotlin\">model = Interpreter(loadModelFile(activity))\n</code></pre>\n<p>Unfortunately using <code>MappedByteBuffer</code> as an argument is already deprecated and will be deleted in future releases but you can solve this problem by delivering ByteBuffer instead and it is as simple as invoking <code>.asReadOnlyBuffer()</code> on <code>loadModelFile</code> method.</p>\n<p>Next step is to read the file with labels. You can easily grab them with:</p>\n<pre><code class=\"language-kotlin\">@Throws(IOException::class)  \nprivate fun getLabels(assetManager: AssetManager, labelPath: String): List&#x3C;String> {  \n    val labels = ArrayList&#x3C;String>()  \n    val reader = BufferedReader(InputStreamReader(assetManager.open(labelPath)))  \n    while (true) {\n        val label = reader.readLine() ?: break\n        labels.add(label)\n    }  \n    reader.close()  \n    return labels  \n}\n</code></pre>\n<p>The last thing is to create a method that will take an image as an argument and return a list of labels with assigned probabilities to them.</p>\n<pre><code class=\"language-kotlin\">fun recognize(bitmap: Bitmap): List&#x3C;Recognition>{\n</code></pre>\n<p>Because our model expects the exact input shape (224x224 pixels) we need to rescale a delivered bitmap to fit into these constraints. </p>\n<pre><code class=\"language-kotlin\">    val scaledBitmap =  Bitmap.createScaledBitmap(bitmap, MODEL_INPUT_SIZE, MODEL_INPUT_SIZE, false)\n</code></pre>\n<p>Next, we need to create byteBuffer of appropriate size that will be passed as an argument to the model.</p>\n<pre><code class=\"language-kotlin\">    val byteBuffer = ByteBuffer  \n        .allocateDirect(  \n                    BATCH_SIZE *         // amount of images per single processing\n                    MODEL_INPUT_SIZE *   // img height\n                    MODEL_INPUT_SIZE *   // img width\n                    BYTES_PER_CHANNEL *  // size of float = 4\n                    PIXEL_SIZE           // r+g+b = 1+1+1\n      )  \n        .apply { order(ByteOrder.nativeOrder()) } // force device's native order (BIG_ENDIAN or LITTLE_ENDIAN)\n</code></pre>\n<p>And load <code>byteByffer</code> with the image data as <em>floating point numbers</em>. In order to decode color (ignoring alpha) in each pixel on a bitmap, we need to mask the least significant 8 bits and its multiple.</p>\n<pre><code class=\"language-kotlin\">    val pixelValues = IntArray(MODEL_INPUT_SIZE * MODEL_INPUT_SIZE)  \n    bitmap.getPixels(pixelValues, 0, bitmap.width, 0, 0, bitmap.width, bitmap.height)  \n  \n    var pixel = 0  \n    for (i in 0 until MODEL_INPUT_SIZE) {  \n        for (j in 0 until MODEL_INPUT_SIZE) {  \n            val pixelValue = pixelValues[pixel++]  \n            byteBuffer.putFloat((pixelValue shr 16 and 0xFF) / 255f)  \n            byteBuffer.putFloat((pixelValue shr 8 and 0xFF) / 255f)  \n            byteBuffer.putFloat((pixelValue and 0xFF) / 255f)  \n        }  \n    }\n</code></pre>\n<p>Finally, we can pass <em>byteBuffer</em> to the model. The interpreter expects for the second argument container for results and it is <em>array</em> of <em>float arrays</em> (<em>array</em> for each image and each one will contain <em>float array</em> of probabilities).</p>\n<pre><code class=\"language-kotlin\">    val results = Array(BATCH_SIZE) { FloatArray(labels.size) }\n    model.run(byteBuffer, results)\n    return parseResults(results)\n}\n</code></pre>\n<p>The last step is to bond probability with a proper class.</p>\n<pre><code class=\"language-kotlin\">private fun parseResults(result: Array&#x3C;FloatArray>): List&#x3C;Recognition> {  \n  \n    val recognitions = mutableListOf&#x3C;Recognition>()  \n  \n    labels.forEachIndexed { index, label ->  \n        val probability = result[0][index]  \n        recognitions.add(Recognition(label, probability))  \n    }  \n  \n  return recognitions.sortedByDescending { it.probability }  \n}\n</code></pre>\n<p>where <em>Recognition</em> is our humble result data class.</p>\n<pre><code class=\"language-kotlin\">data class Recognition(  \n    val name: String,  \n    val probability: Float  \n) {  \n    override fun toString() =  \n        \"$name : ${probability*100}%\"  \n}\n</code></pre>\n<h6>Don't forget that there are many things you should consider to make it work well i.e. handling a camera orientation or using post-training quantization if you need higher speed with a bit lower accuracy and lighter.</h6>\n<h2>It‚Äôs showtime!</h2>\n<p>The above code is a minimalistic version for getting TFLite solving for us <em>image classification</em> problem. With the provided model you can successfully classify all photos that are in this blog post. üì∏\nYou can find the demo <a href=\"https://github.com/ares97/tflitedemo-mobilenetv2-imagenet-classification\">here</a>.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b25a58e6fcfcfc260f85070b4d39898e/314c6/windsurf.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.08108108108109%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAABAABA//EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAEPHF2EmR//xAAZEAACAwEAAAAAAAAAAAAAAAAAAgMSFAH/2gAIAQEAAQUCzVMzmdizEc0hKvLf/8QAFREBAQAAAAAAAAAAAAAAAAAAEBL/2gAIAQMBAT8Bo//EABURAQEAAAAAAAAAAAAAAAAAABAR/9oACAECAQE/AYf/xAAaEAACAgMAAAAAAAAAAAAAAAAAARAxERIy/9oACAEBAAY/AtspqLRbOij/xAAcEAADAAIDAQAAAAAAAAAAAAAAAREhYTFBkcH/2gAIAQEAAT8hTJtGhpzMCavqJXHsUShm4rXR/9oADAMBAAIAAwAAABBjD//EABcRAAMBAAAAAAAAAAAAAAAAAAABESH/2gAIAQMBAT8Q0oVn/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQIBAT8QI//EABoQAQEAAwEBAAAAAAAAAAAAAAERACExUZH/2gAIAQEAAT8QoxxJfp8wyQso75lxDPMUJpGvM7GDewcJMALErvzP/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"windsurf\"\n        title=\"windsurf\"\n        src=\"/static/b25a58e6fcfcfc260f85070b4d39898e/1c72d/windsurf.jpg\"\n        srcset=\"/static/b25a58e6fcfcfc260f85070b4d39898e/a80bd/windsurf.jpg 148w,\n/static/b25a58e6fcfcfc260f85070b4d39898e/1c91a/windsurf.jpg 295w,\n/static/b25a58e6fcfcfc260f85070b4d39898e/1c72d/windsurf.jpg 590w,\n/static/b25a58e6fcfcfc260f85070b4d39898e/a8a14/windsurf.jpg 885w,\n/static/b25a58e6fcfcfc260f85070b4d39898e/fbd2c/windsurf.jpg 1180w,\n/static/b25a58e6fcfcfc260f85070b4d39898e/314c6/windsurf.jpg 5735w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>","excerpt":"As I've already listed in my recent blog post there are lots of advantages of making inference directly on a mobile device instead of using‚Ä¶","frontmatter":{"slug":null,"title":"Image classification with TensorFlow Lite on Android","description":null,"author":"radeks","tags":["android","tensorflow lite","deep learning","image classification"],"date":"2019-05-05T22:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#082838","images":{"fallback":{"src":"/static/93d5e5f8092ddab1c61b71333f6f586a/934d3/surf1.jpg","srcSet":"/static/93d5e5f8092ddab1c61b71333f6f586a/535e4/surf1.jpg 721w,\n/static/93d5e5f8092ddab1c61b71333f6f586a/fdbc7/surf1.jpg 1443w,\n/static/93d5e5f8092ddab1c61b71333f6f586a/934d3/surf1.jpg 2885w","sizes":"(min-width: 2885px) 2885px, 100vw"},"sources":[{"srcSet":"/static/93d5e5f8092ddab1c61b71333f6f586a/56c96/surf1.webp 721w,\n/static/93d5e5f8092ddab1c61b71333f6f586a/44bc6/surf1.webp 1443w,\n/static/93d5e5f8092ddab1c61b71333f6f586a/2d8bf/surf1.webp 2885w","type":"image/webp","sizes":"(min-width: 2885px) 2885px, 100vw"}]},"width":2885,"height":1664}}}},"timeToRead":5,"fileAbsolutePath":"/home/runner/work/new-www/new-www/src/mdData/blog/2019-05-06-image-classification-tensorflowlite-android.md"},"site":{"siteMetadata":{"siteUrl":"https://brightinventions.pl"}}},"pageContext":{"fileAbsolutePath":"/home/runner/work/new-www/new-www/src/mdData/blog/2019-05-06-image-classification-tensorflowlite-android.md"}},"staticQueryHashes":["2189233960","3181594896"]}